# 1 Quali sono i principali paradigmi del ML? chiarire le differenze, e dare particolare enfasi per il caso del supervised learning. Si distinguano classificazione e regressione.
I principali paradigmi del machine learning sono: **Supervised Learning, Unsupervised Learning e Reinforcement Learning**
- Il **_supervised learning_*** agisce in base all'attività: l'algoritmo prevede il comportamento di un *agente*, utilizzando l'esperienza del passato tramite algoritmi di regressione e classificazione. L'obiettivo è che l'algoritmo fornisca una risposta corretta a ogni esempio nel dataset. Presuppone che i dati siano delle coppie $(x, y)$ e ci sia una funzione di apprendimento che cattura le informazioni da ogni esempio. Quindi viene costruita una funzione $h$ che data in input una $x$ deve produrre il corrispettivo $y$ correttamente. Quindi all'inizio deve mappare, dai dati forniti, tutti i tipi di $x$ con la corrispettiva $y$ dal dataset fornito per il training. L'output è diverso in base all'algoritmo implementato che normalmente può essere un algoritmo di *classificazione o regressione*. L'esperienza fornita dal dataset da sola non ci permette di fare previsioni sulle istanze di dati non visti.
- Il **_unsupervised learning_** agisce in base ai dati: l'algoritmo individua similitudini e strutture nascoste all'interno dei dati (*clustering*). Per cui il goal è trovare la regolarità o patterns sui dati. Vengono forniti degli esempi $x$, e si cercano le regolarità su tutto il dominio dell'input
- Il **_reinforcement learning_** agisce in base all'ambiente: l'algoritmo impara a reagire all'ambiente e a tenere comportamenti intelligenti. La differenza tra i paradigmi precedenti è che c'è uno scenario di *agenti autonomi*, abbiamo uno *scenario attivo* e non invece un *dataset fisso* da cui apprendere in modo statico. Si osserva l'ambiente e si apprende da esso e da ciò che succede, si va a misurare come l'agente si muove nell'ambiente in modo corretto o sbagliato. L'obiettivo è di ottimizzare la funzione, ovvero le task che deve produrre. L'agente ha degli stati in cui può essere e delle azioni che può eseguire nell'ambiente e ha delle "*reward*" in base alle azioni che esegue che possono essere positive, neutrali o negative da cui apprendere.
Un algoritmo di **Classificazione** separa i dati in 2o+ *classi*. Quando fornisco un esempio al classificatore, l'algoritmo mi restituisce la classe a cui potrebbe appartenere. Gli output categorici sono chiamati labels o classi, e possiamo avere classificazioni che possono essere *binarie* o *multi-class* ovvero con 3 o più classi da individuare. Inoltre abbiamo classificatori *multi-label* in cui andiamo ad allenare un nostro classificatore a riconoscere un determinato tipo di classe *individualmente*. Per esempio da un immagine di una piazza dobbiamo riconoscere in una immagine in comune diversi oggetti. I classificatori si differenziano in:
- **Lineari**: sono semplici e veloci ma risentono del problema dell'**underfiting;** Uno spazio, come per esempio un iperpiano che viene separato da una linea che indica il confine tra 2 spazi
- **Non Lineari**: sono più precisi ma più lenti da elaborare e c'è sempre il rischio di cadere nell'**overfitting**
Un algoritmo di **Regressione** si basa sull'interpolazione dei dati per associare tra loro 2 o più caratteristiche (**feature**). Quando forniscono all'algoritmo una caratteristica in input mi restituisce l'altra caratteristica. Come i classificatori abbiamo regressori lineari e non lineari.
- **La regressione lineare** è un approccio **lineare** che modella la relazione **tra** una variabile dipendente e una o più variabili indipendenti. Non è molto utile utilizzarlo per problemi di classificazione.
- **La regressione non lineare** ha una accuratezza del modello previsionale più alta rispetto ai regressori lineari perchè la stima è una curva o uno spazio curvo. Tuttavia si rischia di cadere in casi di **overfitting**. Essa è definita dalla notazione: $h_\theta(x) = \theta \times x = \theta^T \times x$ 
- **La regressione Logistica** è un modello **statistico** che predice la probabilità di un risultato che **può avere solo 2 valori**
La funzione di predizione $h(x)$ produce risultati compresi tra 0 e 1. Questo permette di considerare questo valore come una probabilità, tramite spesso una funzione come la sigmoide che è ideale per task di classificazione.

# Cosa si intende per "one learning algorithm hypothesis" e come tale ipotesi si relazione con le reti neurali artificiali? Si fornisca inoltre una descrizione esaustiva degli elementi/ingredienti principali che permettono la definizione di una rete multistrato
Ci sono evidenze che il cervello umano utilizzi lo stesso algoritmo di apprendimento per processare diversi input, i quali possono essere per esempio la vista, l'udito e il tatto attraverso i neuroni.
L'idea è che se prendiamo l'area del cervello adibita all'orecchio, ovvero i suoi neuroni e i suoi collegamenti e la potessimo tagliare e ricollegare per esempio per usarla per un altro senso come la vista, il cervello col tempo si riconfigura, cercando di interpretare il nuovo segnale che gli arriva per svolgere il nuovo compito assegnato.
L'idea della **rete neurale** è questa, non c'è bisogno di creare 100 algoritmi per 100 compiti diversi, ma di creare un **modello semplice** che poi verrà adattato al compito assegnato, ma ovviamente questa è un idea filosofica.
La più piccola unità di una rete neurale è il **neurone**, esso ottiene molteplici dati in input tramite una funzione matematica produce un output. Il **neurone** è composto da: **input** che rappresentano i dentriti del neurone umano, l'elaborazione effettuata tramite funzioni matematiche che rappresentano il nucleo e l'output che corrisponde all'assone del neurone umano.
Storicamente il primo esempio è stato il **percettrone** il quale tramite una combinazione lineare $h_\theta(x)$ dava in output 1 se $\theta^T \times x$ era maggiore di 0, altrimenti 0. La funzione applicata ad $x$, può essere per esempio la **Logistic Unit**, ma anche la **Sigmoide** o la **Tahn**, ma anche tante altre. Idealmente la rete neurale è un gruppo di diversi neuroni con forti collegamenti tra di essi Tendenzialmente avremo 3 livelli: **input layer, hidden layer e output layer**.
L'**architettura** della rete neurale non è altro che una scelta dei 3 tipi di neuroni descritti sopra. Idealmente l'unica scelta architetturale importante è sull'hidden layer, poichè l'input, che sono le features dipendono dall'hardware disponibile e dalla complessità del problema e l'output normalmente è già stabilito a priori.
Nell'_hidden layer_ scegliere quanti neuroni ha ogni strato, muoversi in profondità e quanti strati porre, ovvero muoversi in larghezza, significa apportare nuovi parametri non è una scelta facile.
Idealmente ogni _neurone_ è una _funzione_, la quale prende in input il risultato del neurone precendente, tranne per il primo strato che prende l'input e lo processa e produce un output che sarà consegnato a un altro gruppo di neuroni. Per cui vi è una **gerarchia** ed è per questo che è utile avere milioni di neuroni, perchè così l'input iniziale verrà processato idealmente milioni di volte prima di essere restituito.
Riguardo all'input di un neurone/percettrone, vi è un **bias unit**, cioè un bias da applicare all'operazione che svolge il neurone. Per esempio, se il neurone svolge una sommatoria la quale serve a la operazione della porta logica & tra solo 2 input corrisponderà a:
	Riceve da 2 input ognuno così: +20 se l'input è 1 o 0 se l'input è 0 e avrà come **bias unit** sempre in input ma non da un output di un altro neurone, il valore -30 il quale sommato ai 2 input ricevuti, se il risultato >> 0 significa che ha ricevuto due 1 in input restituisca 1, se il risultato <<0 significa che almeno un input è 0 e restituisce 0. Per cui questo **bias unit** serve a svolgere le operazioni dei neuroni.
Prendendo un esempio banale come l'operazione logica XNOR la quale corrisponde all'unione delle operazioni logiche &, NOT e OR, essa suddivide il piano cartesiano in 4 parti, e questa suddivisone non può essere rappresentata da un singolo neurone con classificazione lineare poichè il singolo neurone non è capace di svolgere tutte queste operazioni logiche insieme.
La soluzione è **combinare** i neuroni con compiti diversi e l'output combinato dai due che effettuano 2 operazioni matematiche diverse rappresenterà l'output desiderato.
È importante sottolineare che generalmente, prendendo d'esempio un problema di classificazione non lineare e dei neuroni con classificazione lineare, se si prende individualmente il risultato dei singoli neuroni, la classificazione sarà terribile, ma collaborando insieme per risolvere un problema complesso, produrranno una suddivisione ottimale, risolvendo sempre per esempio per un problema non lineare. Per cui una rete neurale non è altro che un insieme di tanti neuroni "stupidi" che svolgono task intermedie, che possono fare operazioni uguali ma con parametri diversi, e tutti gli output vengono sommati insieme per produrre una soluzione a un problema complesso.

# 3 Si descriva in modo dettagliato il modello di logistic regression (con regolarizzazione), le sue principali caratteristiche ed il contributo dei diversi elementi presenti nella funzione di costo. Si riporti infine una descrizione accurata delle differenze di tale modello rispetto ad un semplice classificatore lineare, anche mediante esermpi qualitativi
La regressione logistica è un algoritmo di apprendimento supervisionato che costruisce un modello probabilistico lineare di classificazione dei dati. È usata nel machine learning per l'addestramento di un algoritmo nella classificazione supervisionata dei dati usato per stimare due risultati e solo 2.
L'algoritmo stima la probabilità di occorrenza di un evento adattando i dati a una funzione logistica.
Essa è ottenuta dall'espressione: 
